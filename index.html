<!DOCTYPE html>
<!-- saved from url=(0039)http://www.cbsr.ia.ac.cn/users/sfzhang/ -->
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0">
  <meta name="author" content="yangxue">
  <title>Ting-Feng Zhao's Homepage</title>

  <!-- CSS  -->
  <link href="./files/materialize.min.css" type="text/css" rel="stylesheet" media="screen,projection">
  <link href="./files/aos.css" type="text/css" rel="stylesheet" media="screen,projection">
  <link href="./files/style.css" type="text/css" rel="stylesheet" media="screen,projection">

  <link rel="shortcut icon" href="./images/tingfengzhao.png">
  <script type="text/javascript" src="./files/jquery-1.12.4.min.js.ä¸‹è½½"></script>
  <style>
    @-moz-keyframes nodeInserted {
      from {
        opacity: 0.99;
      }

      to {
        opacity: 1;
      }
    }

    @-webkit-keyframes nodeInserted {
      from {
        opacity: 0.99;
      }

      to {
        opacity: 1;
      }
    }

    @-o-keyframes nodeInserted {
      from {
        opacity: 0.99;
      }

      to {
        opacity: 1;
      }
    }

    @keyframes nodeInserted {
      from {
        opacity: 0.99;
      }

      to {
        opacity: 1;
      }
    }

    embed,
    object {
      animation-duration: .001s;
      -ms-animation-duration: .001s;
      -moz-animation-duration: .001s;
      -webkit-animation-duration: .001s;
      -o-animation-duration: .001s;
      animation-name: nodeInserted;
      -ms-animation-name: nodeInserted;
      -moz-animation-name: nodeInserted;
      -webkit-animation-name: nodeInserted;
      -o-animation-name: nodeInserted;
    }
  </style>

</head>

<body data-aos-easing="ease" data-aos-duration="400" data-aos-delay="0">

  <div class="navbar-fixed">

    <nav class="white">
      <div class="nav-wrapper container"><a id="logo-container" href="https://2112529.github.io/#"
          class="brand-logo"></a>
        <ul class="left">
          <li><a class="nav-item waves-effect waves-light active" href="https://2112529.github.io/#home">Home</a></li>
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#biography">Biography</a>
          </li>
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#news">News</a></li>
          <li><a class="nav-item waves-effect waves-light"
              href="https://2112529.github.io/#recent_works">Publications</a></li>
          <!-- <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#preprints">Preprints</a></li> -->
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#activities">Activities</a>
          </li>
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#education">Education</a>
          </li>
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#internship">Internship</a>
          </li>
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#awards">Awards</a></li>
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#demos">Demos</a></li>
          <li><a class="nav-item waves-effect waves-light" href="https://2112529.github.io/#people">People</a></li>
        </ul>
        </ul>

      </div>
    </nav>
  </div>


  <!--==========================================
                   Profile
===========================================-->

  <div id="home" class="parallax-container scrollspy">


    <div class="container row cover-block">

      <div class="profile-image-block col s12 m12 l4 center">
        <img class="responsive-img profile-photo z-depth-2" src="./images/yangxue.jpg">
      </div>

      <div class="profile-content-block col s12 m12 l8">
        <h5 class="profile-name">Ting-Feng Zhao (èµµå»·æ«)</h5>

        <hr>
        <h6 class="profile-link"><strong>Master Student</strong> at <a
            href="https://www.nankai.edu.cn/"></a><strong>NanKai University</strong></a>, <a
            href="https://mmcheng.net/"><strong>VCIP Laboratory</strong></a></h6>
        <h6 class="profile-link"><strong>Address</strong>: 38 Tongyan Road, Haihe Education Park, Tianjin, China, China
        </h6>
        <h6 class="profile-link"><strong>Email</strong>: 2112529@mail.nankai.eud.cn</h6>

        <h1></h1>

        <!-- <a href="https://scholar.google.com/citations?user=2xTlvV0AAAAJ&hl=en" target="_blank"><img
            class="responsive-img social-photo " src="./images/google_scholar.jpg"></a> -->

        <a href="https://github.com/2112529" target="_blank"><img class="responsive-img social-photo "
            src="./images/github.jpg"></a>

        <!-- <a href="./files/YangXue-CV.pdf" target="_blank"><img class="responsive-img social-photo "
            src="./images/cv.png"></a> -->

        <a href="./images/Wechat_QR_code.jpg" target="_blank"><img class="responsive-img social-photo "
            src="./images/wechat.png"></a>

        <!-- <a href="https://www.zhihu.com/people/2112529" target="_blank"><img class="responsive-img social-photo "
            src="./images/zhihu1.png"></a> -->
        <!-- 
        <a href="https://www.linkedin.com/in/%E5%AD%A6-%E6%9D%A8-43065a1b4/" target="_blank"><img
            class="responsive-img social-photo " src="./images/linkedin.png"></a> -->

        <!-- <a href="https://www.researchgate.net/profile/Xue_Yang87" target="_blank"><img
            class="responsive-img social-photo " src="./images/rg.png"></a> -->

      </div>

    </div>

    <div class="parallax"><img src="./images/homepage_bg1.jpeg" alt="Unsplashed background img 1"
        style="display: block; transform: translate3d(-50%, 153px, 0px);"></div>

  </div>



  <!--==========================================
                   About
===========================================-->
  <div class="section about-section scrollspy" id="biography">

    <div class="row container">
      <br><br>
      <div class="row">
        <div class="title">ğŸ‡¨ğŸ‡³ Biography</div>
        <hr>
      </div>

      <div class="row">
        <p>
          Ting-Feng Zhao is now a Researcher at <a href="https://opengvlab.shlab.org.cn/">OpenGVLab</a>, <a
            href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>, collaborated with <a
            href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and <a
            href="https://scholar.google.com/citations?hl=zh-CN&user=02RXI00AAAAJ">Dr. Xizhou Zhu</a>. Ting-Feng Zhao's
          research interests include Deep Learning and Computer Vision, with a focus on Generic/Oriented Object
          Detection/Instance Segmentation, AI Agent, Vision-Language Models.
        </p>
        <!-- <p>
          Ting-Feng Zhao received the B. E. degree from School of Information Science and Engineering, <a
            href="http://www.csu.edu.cn/">Central South University</a>, Hunan, China, in 2016. He received the M. S.
          degree from <a href="https://eece.ucas.ac.cn/index.php/zh-cn/">School of Electronic, Electrical and
            Communication Engineering</a>, <a href="https://www.ucas.ac.cn/">University of Chinese Academy of
            Sciences</a>,
          Beijing, China, in 2019. Ting-Feng Zhao obtained the Ph.D. degree from <a
            href="https://news.sjtu.edu.cn/jdyw/20190930/111855.html">Wu Honor Class</a> (<a
            href="https://xsb.seiee.sjtu.edu.cn/xsb/info/35105.htm">å´æ–‡ä¿Šäººå·¥æ™ºèƒ½åšå£«ç­</a>), <a
            href="http://www.cs.sjtu.edu.cn">Department of Computer Science and Engineering</a>, <a
            href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, Shanghai, China, in 2023. His research
          advisor is <a href="http://thinklab.sjtu.edu.cn/">Prof. Junchi Yan</a>.
        </p> -->
        <!-- <p>
          Ting-Feng Zhao has published about 50 papers <a
            href="https://scholar.google.com/citations?user=2xTlvV0AAAAJ&hl=en"><img
              src="https://img.shields.io/endpoint?logo=Google%20Scholar&amp;url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2F2112529%2F2112529.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&amp;labelColor=f6f6f6&amp;color=9cf&amp;style=flat&amp;label=citations" /></a>
          at the top-tier international CV/ML/AI conferences and journals, such as TPAMI, IJCV, CVPR, ECCV, ICCV, ICML,
          NeurIPS, ICLR, AAAI and ACM MM. He is also the leading contributor to the <a
            href="https://github.com/open-mmlab/mmrotate">MMRotate</a> <img
            src="https://img.shields.io/github/stars/open-mmlab/mmrotate?style=social" />, <a
            href="https://github.com/2112529/RotationDetection">AlphaRotate</a> <img
            src="https://img.shields.io/github/stars/2112529/RotationDetection?style=social" /> and <a
            href="https://github.com/Jittor/JDet">JDet</a> <img
            src="https://img.shields.io/github/stars/Jittor/JDet?style=social" /> open-source projects for oriented
          object detection, and with 8000+ stars in Github.
          Ting-Feng Zhao won <a href="https://www.gs.sjtu.edu.cn/yxbslw" , target="_blank">SJTU Outstanding Doctoral
            Dissertation</a> (2023), <a
            href="https://www.ccf.org.cn/Membership/Individual_member/Honor/yxbsxwlwjljh/2024-01-05/811519.shtml"
            target="_blank">CCF Outstanding Doctoral Dissertation Award</a> (2023), <a
            href="https://mp.weixin.qq.com/s/mUgpVmyvCHdo5-T6-u8Yxg">CCF-CV Academic Emerging Scholar</a> (2022), <a
            href="https://www.seiee.sjtu.edu.cn/xsgz_tzgg_zyfz/7915.html" target="_blank">Shanghai Outstanding
            Graduates</a> (2023), <a href="https://mp.weixin.qq.com/s/liVosHsotD2zDMyfmTIQJg" target="_blank">Doctoral
            National Scholarship</a> (2021/2022), <a href="https://mp.weixin.qq.com/s/hr7qtx3OUffSGS9qUhqc9w"
            target="_blank">SJTU Scholar Star Nomination Award</a> (2021), and also selected into <a
            href="https://mp.weixin.qq.com/s/sBczTcu0WN4nYpouOm5RMQ" target="_blank">the 10th Young Talent Support
            Project funded by CAST</a> (2024), and the <a
            href="https://topresearcherslist.com/Home/Profile/823455">World's Top 2% Scientists List</a> (2023-2024).
        </p> -->

        <!-- <p>
          <font color="red">
            <i>
              <b>æˆ‘å°†äº 2025 å¹´æ˜¥å­£åŠ å…¥ä¸Šæµ·äº¤é€šå¤§å­¦ <a href="http://thinklab.sjtu.edu.cn/"><strong>ReThinkLab</strong></a>
                æ‹…ä»»åŠ©ç†æ•™æˆã€‚</b><br>
              <b>æˆ‘æ­£åœ¨å¯»æ‰¾è‡ªé©±åŠ›è¾ƒå¼ºçš„å­¦ç”Ÿï¼ˆç¡•å£« 2025 å¹´æ˜¥å­£å’Œç§‹å­£ï¼Œåšå£« 2026 å¹´æ˜¥å­£å’Œç§‹å­£ï¼‰ã€å®ä¹ ç”Ÿ/è®¿é—®å­¦è€…ï¼Œä¸<a
                  href="http://thinklab.sjtu.edu.cn/">ä¸¥éªé©°æ•™æˆ</a>å…±åŒæŒ‡å¯¼ï¼Œç›®æ ‡æ˜¯åœ¨è®¡ç®—æœºè§†è§‰ã€å¤šæ¨¡æ€æ¨¡å‹ã€é¥æ„Ÿå›¾åƒè§£è¯‘ç­‰è¯¾é¢˜ä¸Šåšå‡ºæœ‰å½±å“åŠ›çš„å·¥ä½œã€‚è¯·éšæ—¶é€šè¿‡ç”µå­é‚®ä»¶ä¸æˆ‘è”ç³»ã€‚</b>
              <br>

              <b>I will be joining the <a href="http://thinklab.sjtu.edu.cn/"><strong>ReThinkLab</strong></a>, Shanghai
                Jiao Tong University as an Assistant Professor in the spring of 2025.</b> <br>
              <b>Looking for self-motivated students (Master 2025 spring & fall, Ph.D. 2026 spring & fall),
                interns/visitors to join us, co-supervised by <a href="http://thinklab.sjtu.edu.cn/">Prof. Junchi
                  Yan</a>, with the goal of doing impactful work on the topic of Computer Vision, Vision-Language
                Models, Remote Sensing (AI4RS), etc. Please do not hesitate to contact me via email.</b>
            </i>
          </font>
        </p> -->
        <!-- <p>
      <font color="red">
        <i>
          I am also looking for a full-time job. 
        </i>
      </font>
      </p> -->
      </div>
    </div>



    <!--==========================================
                   News
===========================================-->
    <div class="section news-section scrollspy" id="news">

      <div class="row container">
        <div class="row">
          <div class="title">ğŸ”¥ News</div>
          <hr>
        </div>
        <div class="row">
          <ul>

            <!-- <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true" class="aos-init aos-animate">
          â€¢ 02 / 2024: &nbsp; <font color="red"> Happy Chinese New Year! </font>
        </li> -->

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 01 / 2025: &nbsp; Two paper on related to PEFT (FLoRA) and OBB (PointPBB-v2) is accepted by <b>ICLR
                2025</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 12 / 2024: &nbsp; I was selected into the 10th Young Talent Support Project funded by CAST <a
                href="https://mp.weixin.qq.com/s/sBczTcu0WN4nYpouOm5RMQ" target="_blank">[ç¬¬åå±Šä¸­å›½ç§‘åé’å¹´äººæ‰æ‰˜ä¸¾å·¥ç¨‹]</a>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 12 / 2024: &nbsp; One paper on related to VLM for RS (DiffClip) is accepted by <b>AAAI 2025</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 12 / 2024: &nbsp; One paper on related to Multi-UAV (UCDNet) is accepted by <b>TGRS</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 11 / 2024: &nbsp; One paper on related to SGG and OBB (STAR) is accepted by <b>TPAMI</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 10 / 2024: &nbsp; Two reports are made at the <a href="https://ccf.org.cn/cncc2024/schedule_d_4240"
                target="_blank">CNCC 2024 Hengdian</a> <a href="https://ccf.org.cn/cncc2024/schedule_d_4240"
                target="_blank">[CCFä¼˜åšçš„åŸ¹å…»ä¸æˆé•¿]</a>, <a href="https://ccf.org.cn/cncc2024/schedule_d_4128"
                target="_blank">[ç«¯å¯æœªæ¥ï¼šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æŠ€æœ¯æ–°è¶‹åŠ¿]</a>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 10 / 2024: &nbsp; è£è·ä¸­å›½å›¾è±¡å›¾å½¢å­¦å­¦ä¼šè§†è§‰å¤§æ•°æ®ä¸“å§”ä¼š(CSIG-BVD)é¢å‘çš„<a
                href="https://mp.weixin.qq.com/s/fhfizq577_-zPu8cGIAOxw" target="_blank">â€œæœåŠ¡è´¡çŒ®å¥–â€</a>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 09 / 2024: &nbsp; ğŸ‰ğŸ‰ğŸ‰ Three papers related to detection (<b>one Oral</b> and <b>one Spotlight</b>)
              are accepted by <b>NeurIPS 2024</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 09 / 2024: &nbsp; I'm selected into the <a
                href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7">World's Top 2% Scientists 2024
                List</a>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 08 / 2024: &nbsp; I will serve as <b>Area Chair</b> for <b>ICLR 2025</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 07 / 2024: &nbsp; Two papers on scene text detection (FreeReal) and open vocabulary detection (CastDet)
              are accepted by <b>ECCV 2024</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 03 / 2024: &nbsp; <a href="https://mp.weixin.qq.com/s/5nb3je2q1ZHqzL8oDE-YUQ" target="_blank">SJTU
                Outstanding Doctoral Dissertation</a>, fifteen winners in SJTU <a
                href="https://www.gs.sjtu.edu.cn/yxbslw">[ä¸Šæµ·äº¤é€šå¤§å­¦ä¼˜ç§€åšå£«å­¦ä½è®ºæ–‡ï¼Œå…¨æ ¡15äºº/å·¥å­¦8äºº]</a>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 02 / 2024: &nbsp; Four papers on oriented object detection, LLM for AI Agent (Minecraft) are accepted by
              <b>CVPR 2024</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 02 / 2024: &nbsp; One collaborative paper on oriented object detection (ARS-DETR) is accepted by
              <b>TGRS</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 12 / 2023: &nbsp; I'm awarded the <a href="https://mp.weixin.qq.com/s/Hk8iKReAUJVB_CraXqWYKg"
                target="_blank">CCF Doctoral Dissertation Award</a>, nine winners in China <a
                href="https://mp.weixin.qq.com/s/8wLaq2SvYF6zPbf_UFI_XQ">[CCF ä¼˜ç§€åšå£«å­¦ä½è®ºæ–‡æ¿€åŠ±è®¡åˆ’ï¼Œå…¨å›½ä¹äºº]</a>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 12 / 2023: &nbsp; One paper on oriented object detection (AlphaRotate) is accepted by <b>ICASSP 2024</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 11 / 2023: &nbsp; I'm selected into the <a
                href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6">World's Top 2% Scientists 2023
                List</a>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 10 / 2023: &nbsp; I'm elected as CSIG-BVD Committee Member at PRCV 2023
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 09 / 2023: &nbsp; One paper on oriented object detection (H2RBox-v2) is accepted by <b>NeurIPS 2023</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 07 / 2023: &nbsp; One collaborative paper on self-supervised text recognition (CCD) is accepted by
              <b>ICCV 2023</b>
            </li>

            <li data-aos="fade-up" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500"
              data-aos-once="true" class="aos-init aos-animate">
              â€¢ 07 / 2023: &nbsp; Researcher at OpenGVLab, Shanghai AI Laboratory, working with <a
                href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ&hl=en">Prof. Jifeng Dai</a> and <a
                href="https://scholar.google.com/citations?hl=zh-CN&user=02RXI00AAAAJ">Dr. Xizhou Zhu</a>
            </li>

            <div align='right'> <a href="./news.html" class="more" target="_blank"><i></i>
                <font color="#A9A9A9" size="2">LEARN MORE>></font>
              </a></div>

          </ul>
        </div>
      </div>
    </div>


    <!--==========================================
                   Recent Works
===========================================-->
    <div class="section preprints-section scrollspy" id="recent_works">

      <div class="row container">
        <div class="row">
          <div class="title">ğŸ“ Recent Works <a href="./publications.html" class="more" target="_blank"><i></i>
              <font color="#A9A9A9">[Full List]</font>
            </a></div>
          <hr>
        </div>

        <div>
          <font color="blue"><b>( <sup>*</sup> indicates equal contribution, <sup>â€ </sup> indicates corresponding
              author, <sup>#</sup> indicates project lead)</b></font>
        </div><br>

        <hr class="publication-hr">

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/ovd.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">Exploiting Unlabeled Data with Multiple Expert Teachers for Open Vocabulary Aerial
              Object Detection and Its Orientation Adaptation</div>
            <div class="paper-author">Yan Li, Weiwei Guo<sup>â€ </sup>, <font color="#0000dd"><b>Ting-Feng Zhao</b></font>
              , Ning
              Liao, Shaofeng Zhang, Yi Yu, Wenxian Yu, Junchi Yan</div>
            <div>
              <a href="https://arxiv.org/abs/2411.02057" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2411.02057-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&cstart=20&pagesize=80&citation_for_view=2xTlvV0AAAAJ:bFI3QPDXJZMC"
                target="_blank"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
                  width="88" height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:bFI3QPDXJZMC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:bFI3QPDXJZMC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/lizzy8587/CastDet?style=social" />
              <a href="https://github.com/lizzy8587/CastDet" target="_blank">[CastDet-PyTorch]</a>
            </div>
          </div>
        </div>


        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/mono_internvl.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language
              Models with Endogenous Visual Pre-training</div>
            <div class="paper-author">Gen Luo<sup>*</sup>, <font color="#0000dd"><b>Ting-Feng Zhao</font>
              <sup>*</sup></b>,
              Wenhan Dou<sup>*</sup>, Zhaokai Wang<sup>*</sup>, Jifeng Dai, Yu Qiao, Xizhou Zhu<sup>â€ </sup>
            </div>
            <div>
              <a href="https://arxiv.org/abs/2410.08202" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2410.08202-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&citation_for_view=2xTlvV0AAAAJ:pyW8ca7W8N0C"
                target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="88"
                  height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:pyW8ca7W8N0C' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:pyW8ca7W8N0C' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/homepage.png">
              <a href="https://internvl.github.io/blog/2024-10-10-Mono-InternVL/" target="_blank">[project page]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/zhihu.png">
              <a href="https://zhuanlan.zhihu.com/p/4063966074" target="_blank">[è§£è¯»]</a>
            </div>
          </div>
        </div>

        <hr class="publication-hr">

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/pointobbv2.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented
              Object Detection</div>
            <div class="paper-author">Botao Ren<sup>*</sup>, <font color="#0000dd"><b>Ting-Feng Zhao</font>
              <sup>*</sup></b>,
              Yi Yu<sup>*</sup>, Junwei Luo, Zhidong Deng<sup>â€ </sup>
            </div>
            <div class="paper-conf">In <em>International Conference on Learning Representations <font color="red">
                  <b>(ICLR, Tsinghua-A)</b>
                </font></em>, Singapore, 2025</div>
            <div>
              <a href="https://arxiv.org/abs/2410.08210" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2410.08210-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&citation_for_view=2xTlvV0AAAAJ:f2IySw72cVMC"
                target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="88"
                  height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:f2IySw72cVMC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:f2IySw72cVMC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/taugeren/PointOBB-v2?style=social" />
              <a href="https://github.com/taugeren/PointOBB-v2" target="_blank">[PointOBB-v2-PyTorch]</a>
            </div>
          </div>
        </div>

        <hr class="publication-hr">


        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/vlgfm.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">Towards Vision-Language Geo-Foundation Model: A Survey</div>
            <div class="paper-author">Yue Zhou, Litong Feng, Yiping Ke, Xue Jiang, Junchi Yan, <font color="#0000dd">
                <b>Ting-Feng Zhao</b>
              </font>, Wayne Zhang</div>
            <div>
              <a href="https://arxiv.org/abs/2406.09385" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2406.09385-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&cstart=20&pagesize=80&citation_for_view=2xTlvV0AAAAJ:4OULZ7Gr8RgC"
                target="_blank"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
                  width="88" height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:4OULZ7Gr8RgC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:4OULZ7Gr8RgC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/zytx121/Awesome-VLGFM?style=social" />
              <a href="https://github.com/zytx121/Awesome-VLGFM" target="_blank">[Awesome-VLGFM]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/zhihu.png">
              <a href="https://zhuanlan.zhihu.com/p/703991933" target="_blank">[è§£è¯»]</a>
            </div>
          </div>
        </div>

        <hr class="publication-hr">


        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/star.gif">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">STAR: A First-Ever Dataset and A Large-Scale Benchmark for Scene Graph Generation
              in Large-Size Satellite Imagery</div>
            <div class="paper-author">Yansheng Li, Linlin Wang<sup>â€ </sup>, Tingzhu Wang<sup>â€ </sup>, <font
                color="#0000dd"><b>Ting-Feng Zhao</b></font><sup>â€ </sup>, Junwei Luo, Qi Wang, Youming Deng, Wenbin
              Wang, Xian
              Sun, Haifeng Li, Bo Dang, Yongjun Zhang, Yi Yu, Junchi Yan</div>
            <div class="paper-conf"><em>IEEE Transactions on Pattern Analysis and Machine Intelligence <font
                  color="red"><b>(TPAMI, CCF-A)</b></font></em>, 2024</div>
            <div>
              <a href="https://arxiv.org/abs/2406.09410" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2406.09410-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&cstart=20&pagesize=80&citation_for_view=2xTlvV0AAAAJ:u_35RYKgDlwC"
                target="_blank"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
                  width="88" height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:u_35RYKgDlwC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:u_35RYKgDlwC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/Zhuzi24/SGG-ToolKit?style=social" />
              <a href="https://github.com/Zhuzi24/SGG-ToolKit" target="_blank">[SGG-ToolKit], </a>
              <img src="https://img.shields.io/github/stars/2112529/STAR-MMRotate?style=social" />
              <a href="https://github.com/2112529/STAR-MMRotate" target="_blank">[STAR-MMRotate], </a>
              <img src="https://img.shields.io/github/stars/Zhuzi24/STAR-MMDetection?style=social" />
              <a href="https://github.com/Zhuzi24/STAR-MMDetection" target="_blank">[STAR-MMDetection]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/zhihu.png">
              <a href="https://zhuanlan.zhihu.com/p/704536129" target="_blank">[è§£è¯»]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/homepage.png">
              <a href="https://linlin-dev.github.io/project/STAR.html" target="_blank">[project page]</a>
            </div>
          </div>
        </div>

        <hr class="publication-hr">


        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/piip.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">Parameter-Inverted Image Pyramid Networks</div>
            <div class="paper-author">Xizhou Zhu<sup>*</sup>, <font color="#0000dd"><b>Ting-Feng Zhao<sup>*</sup></b>
              </font>,
              Zhaokai Wang<sup>*</sup>, Hao Li, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai<sup>â€ </sup></div>
            <div class="paper-conf"><em>Advances in Neural Information Processing Systems <font color="red"><b>(NeurIPS,
                    CCF-A)</b></font></em>, Vancouver, Canada, <font color="red"><b>Spotlight</b></font>, 2024</div>
            <div>
              <a href="https://arxiv.org/abs/2406.04330" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2406.04330-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&cstart=20&pagesize=80&citation_for_view=2xTlvV0AAAAJ:zA6iFVUQeVQC"
                target="_blank"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
                  width="88" height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:zA6iFVUQeVQC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:zA6iFVUQeVQC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/OpenGVLab/PIIP?style=social" />
              <a href="https://github.com/OpenGVLab/PIIP" target="_blank">[PIIP-PyTorch]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/zhihu.png">
              <a href="https://zhuanlan.zhihu.com/p/705734540" target="_blank">[è§£è¯»]</a>
            </div>
          </div>
        </div>


      </div>

    </div>

    <!--==========================================
                    PEFT
===========================================-->
    <div class="section preprints-section scrollspy" id="llms_and_agent">

      <div class="row container">
        <div class="row">
          <div class="title">ğŸ“ PEFT</div>
          <hr>
        </div>

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/flora.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">FLoRA: Low-Rank Core Space for N-dimension</div>
            <div class="paper-author">Chongjie Si<sup>*</sup>, Xuehui Wang<sup>*</sup>, <font color="#0000dd"><b>Xue
                  Yang</b></font>, Zhengqin Xu, Qingyun Li, Jifeng Dai, Yu Qiao, Xiaokang Yang, Wei Shen<sup>â€ </sup>
            </div>
            <div class="paper-conf">In <em>International Conference on Learning Representations <font color="red">
                  <b>(ICLR, Tsinghua-A)</b>
                </font></em>, Singapore, 2025</div>
            <div>
              <a href="https://arxiv.org/abs/2405.14739" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2405.14739-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&cstart=20&pagesize=80&citation_for_view=2xTlvV0AAAAJ:ZHo1McVdvXMC"
                target="_blank"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
                  width="88" height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:ZHo1McVdvXMC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:ZHo1McVdvXMC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/SJTU-DeepVisionLab/FLoRA?style=social" />
              <a href="https://github.com/SJTU-DeepVisionLab/FLoRA" target="_blank">[FLoRA-PyTorch]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/zhihu.png">
              <a href="https://zhuanlan.zhihu.com/p/705909337" target="_blank">[è§£è¯»]</a>
            </div>
          </div>
        </div>

        <hr class="publication-hr">

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/mona.gif">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">5%>100%: Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition
              Tasks</div>
            <div class="paper-author">Dongshuo Yin<sup>*</sup>, Leiyi Hu<sup>*</sup>, Bin Li, Youqun Zhang, <font
                color="#0000dd"><b>Ting-Feng Zhao<sup>â€ </sup></b></font>
            </div>
            <div>
              <a href="https://arxiv.org/abs/2408.08345" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2408.08345-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&citation_for_view=2xTlvV0AAAAJ:D03iK_w7-QYC"
                target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="88"
                  height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:D03iK_w7-QYC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:D03iK_w7-QYC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/Leiyi-Hu/mona?style=social" />
              <a href="https://github.com/Leiyi-Hu/mona" target="_blank">[Mona-PyTorch]</a>
            </div>
          </div>
        </div>


      </div>

    </div>


    <!--==========================================
    Scene Text Detection and Recognition
===========================================-->
    <div class="section preprints-section scrollspy" id="ocr">

      <div class="row container">
        <div class="row">
          <div class="title">ğŸ“ Scene Text Detection and Recognition <a href="./publications.html" class="more"
              target="_blank"><i></i>
              <font color="#A9A9A9">[Full List]</font>
            </a></div>
          <hr>
        </div>

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/freereal.gif">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors</div>
            <div class="paper-author">Tongkun Guan, Wei Shen<sup>â€ </sup>, <font color="#0000dd"><b>Ting-Feng Zhao</b>
              </font>,
              Xuehui Wang, Xiaokang Yang</div>
            <div class="paper-conf">In <em>Proceedings of the European Conference on Computer Vision <font color="red">
                  <b>(ECCV, CCF-B, Tsinghua-A)</b>
                </font></em>, MiCo Milano, Italy, 2024</div>
            <div>
              <a href="https://arxiv.org/abs/2312.05286" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2312.05286-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&citation_for_view=2xTlvV0AAAAJ:yD5IFk8b50cC"
                target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="88"
                  height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:yD5IFk8b50cC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:yD5IFk8b50cC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/SJTU-DeepVisionLab/FreeReal?style=social" />
              <a href="https://github.com/SJTU-DeepVisionLab/FreeReal" target="_blank">[FreeReal-PyTorch]</a>
            </div>
          </div>
        </div>

        <hr class="publication-hr">

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/ccd.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">Self-supervised Character-to-Character Distillation for Text Recognition</div>
            <div class="paper-author">Tongkun Guan, Wei Shen<sup>â€ </sup>, <font color="#0000dd"><b>Ting-Feng Zhao</b>
              </font>,
              Qi Feng, Zekun Jiang, Xiaokang Yang</div>
            <div class="paper-conf">In <em>Proceedings of the IEEE International Conference on Computer Vision <font
                  color="red"><b>(ICCV, CCF-A)</b></font></em>, Paris, France, 2023</div>
            <div>
              <a href="https://arxiv.org/abs/2211.00288" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2111.06677-B31B1B.svg" /></a>
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&citation_for_view=2xTlvV0AAAAJ:GnPB-g6toBAC"
                target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="88"
                  height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:GnPB-g6toBAC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:GnPB-g6toBAC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/TongkunGuan/CCD?style=social" />
              <a href="https://github.com/TongkunGuan/CCD" target="_blank">[CCD-PyTorch]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/zhihu.png">
              <a href="https://zhuanlan.zhihu.com/p/644350078" target="_blank">[è§£è¯»]</a>
            </div>
          </div>
        </div>

        <hr class="publication-hr">

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <img class="responsive-img" src="./images/gten.png">
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <div class="paper-title">Self-supervised Implicit Glyph Attention for Text Recognition</div>
            <div class="paper-author">Tongkun Guan, Chaochen Gu<sup>â€ </sup>, Jingzheng Tu, <font color="#0000dd"><b>Xue
                  Yang</b></font>, Qi Feng, Yudi Zhao, Wei Shen<sup>â€ </sup></div>
            <div class="paper-conf">In <em>Proceedings of the IEEE Computer Vision and Pattern Recognition <font
                  color="red"><b>(CVPR, CCF-A)</b></font></em>, Vancouver, Canada, 2023</div>
            <div>
              <a href="https://arxiv.org/abs/2203.03382" target="_blank"><img
                  src="http://img.shields.io/badge/cs.CV-arXiv%3A2203.03382-B31B1B.svg" /></a>
              <!-- <a href="https://www.semanticscholar.org/paper/A-Glyph-driven-Topology-Enhancement-Network-for-Guan-Gu/1eb445c6ccb839002b22d6ba05a6d3a83aac8372" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1eb445c6ccb839002b22d6ba05a6d3a83aac8372%3Ffields%3DcitationCount" /></a> -->
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=2xTlvV0AAAAJ&citation_for_view=2xTlvV0AAAAJ:isC4tDSrTZIC"
                target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="88"
                  height="20" role="img">
                  <linearGradient id="s" x2="0" y2="100%">
                    <stop offset="0" stop-color="#bbb" stop-opacity=".1" />
                    <stop offset="1" stop-opacity=".1" />
                  </linearGradient>
                  <clipPath id="r">
                    <rect width="88" height="20" rx="3" fill="#fff" />
                  </clipPath>
                  <g clip-path="url(#r)">
                    <rect width="51" height="20" fill="#555" />
                    <rect x="51" width="37" height="20" fill="#007ec6" />
                    <rect width="88" height="20" fill="url(#s)" />
                  </g>
                  <g fill="#fff" text-anchor="middle" font-family="Verdana,Geneva,DejaVu Sans,sans-serif"
                    text-rendering="geometricPrecision" font-size="110"><text aria-hidden="true" x="265" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)" textLength="410">citations</text><text
                      x="265" y="140" transform="scale(.1)" fill="#fff" textLength="410">citations</text><text
                      class='show_paper_citations' data='2xTlvV0AAAAJ:isC4tDSrTZIC' aria-hidden="true" x="685" y="150"
                      fill="#010101" fill-opacity=".3" transform="scale(.1)"></text><text class='show_paper_citations'
                      data='2xTlvV0AAAAJ:isC4tDSrTZIC' x="685" y="140" transform="scale(.1)" fill="#fff"></text></g>
                </svg></a>
            </div>
            <div>
              <img src="https://img.shields.io/github/stars/TongkunGuan/SIGA?style=social" />
              <a href="https://github.com/TongkunGuan/SIGA" target="_blank">[SIGA-PyTorch]</a>
            </div>
            <div>
              <img class="responsive-img icon" src="./images/zhihu.png">
              <a href="https://zhuanlan.zhihu.com/p/644350078" target="_blank">[è§£è¯»]</a>
            </div>
          </div>
        </div>

      </div>

    </div>



    <!-- ==========================================
                   Activities
=========================================== -->
    <div class="section activities-section scrollspy" id="activities">
      <div class="row container">
        <div class="row">
          <div class="title">ğŸ“š Academic Activities</div>
          <hr>

          <h5>Conference Area Chair/Reviewer</h5>
          <ul>
            <li>â€ƒ â€¢ IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). <b>Reviewer</b>: 2021-2025
            </li>
            <li>â€ƒ â€¢ IEEE/CVF International Conference on Computer Vision (ICCV). <b>Reviewer</b>: 2023</li>
            <li>â€ƒ â€¢ European Conference on Computer Vision (ECCV). <b>Reviewer</b>: 2022/2024</li>
            <li>â€ƒ â€¢ Asian Conference on Computer Vision (ACCV). <b>Reviewer</b>: 2024</li>
            <li>â€ƒ â€¢ Neural Information Processing Systems (NeurIPS). <b>Reviewer</b>: 2022-2024</li>
            <li>â€ƒ â€¢ International Conference on Machine Learning (ICML). <b>Reviewer</b>: 2022-2025</li>
            <li>â€ƒ â€¢ International Conference on Learning Representations (ICLR). <b>AC</b>: 2025; <b>Reviewer</b>: 2024
            </li>
            <li>â€ƒ â€¢ AAAI Conference on Artificial Intelligence (AAAI). <b>Reviewer</b>: 2022-2025</li>
            <li>â€ƒ â€¢ ACM International Conference on Multimedia (ACM MM). <b>Reviewer</b>: 2021-2024</li>
          </ul>

          <h5>Journal Reviewer</h5>
          <ul>
            <li>â€ƒ â€¢ IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
            <li>â€ƒ â€¢ International Journal of Computer Vision (IJCV)</li>
            <li>â€ƒ â€¢ IEEE Transactions on Image Processing (TIP)</li>
            <li>â€ƒ â€¢ Pattern Recognition (PR)</li>
            <li>â€ƒ â€¢ IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
            <li>â€ƒ â€¢ IEEE Transactions on Multimedia (TMM)</li>
            <li>â€ƒ â€¢ IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
            <li>â€ƒ â€¢ IEEE Transactions on Geoscience and Remote Sensing (TGRS)</li>
            <li>â€ƒ â€¢ IEEE Geoscience and Remote Sensing Letters (GRSL)</li>
            <li>â€ƒ â€¢ IEEE Transactions on Intelligent Transportation Systems (TITS)</li>
            <li>â€ƒ â€¢ IEEE Transactions on Multimedia Computing Communications and Applications (TOMM)</li>
            <li>â€ƒ â€¢ Neurocomputing </li>
            <li>â€ƒ â€¢ Remote Sensing</li>
          </ul>

          <h5>Tech. Talks</h5>
          <ul>
            <li>â€ƒ â€¢ 10 / 2024: &nbsp; <a href="https://ccf.org.cn/cncc2024/schedule_d_4240" target="_blank">CNCC 2024
                Hengdian</a> <a href="https://ccf.org.cn/cncc2024/schedule_d_4240" target="_blank">[CNCC 2024 æ¨ªåº—,
                CCFä¼˜åšçš„åŸ¹å…»ä¸æˆé•¿]</a>, <a href="https://ccf.org.cn/cncc2024/schedule_d_4128"
                target="_blank">[ç«¯å¯æœªæ¥ï¼šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æŠ€æœ¯æ–°è¶‹åŠ¿]</a> </li>
            <li>â€ƒ â€¢ 06 / 2023: &nbsp; Valse 2023 Wuxi <a href="http://valser.org/2023/#/workshop" target="_blank">[Valse
                2023 æ— é”¡, W2ï¼šé¥æ„Ÿå½±åƒè§£è¯‘]</a>, <a href="./files/VALSE 2023 Wuxi.pdf" target="_blank">[æŠ¥å‘ŠPPT]</a>, <a
                href="https://www.bilibili.com/video/BV1yu4y1R7f6/?spm_id_from=333.788&vd_source=be89b0c65eab470db155c6731fcffddf">[æŠ¥å‘Šå›æ”¾]</a>
            </li>
            <li>â€ƒ â€¢ 03 / 2023: &nbsp; OpenMMLab <a href="https://mp.weixin.qq.com/s/eoSxS64EtyF8FRMd3BlS-w"
                target="_blank">[OpenMMLabç¤¾åŒºå¼€æ”¾éº¦]</a>, <a href="./files/OpenMMLabå¼€æ”¾éº¦20230302.pdf"
                target="_blank">[slides]</a>, <a href="https://www.bilibili.com/video/BV1GD4y1g7s8"
                target="_blank">[bilibili playback]</a><img
                src="https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1GD4y1g7s8" />,
              <a href="https://www.zhihu.com/zvideo/1614738654315995136" target="_blank">[zhihu playback]</a><img
                src="https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1614738654315995136" />
            </li>
            <li>â€ƒ â€¢ 12 / 2022: &nbsp; Doctoral Forum of PRCV 2022 <a
                href="https://m.inmuu.com/v1/live/news/2265574?gatherId=4043" target="_blank">[PRCV 2022 åšå£«ç”Ÿè®ºå›]</a></li>
            <li>â€ƒ â€¢ 07 / 2022: &nbsp; OpenMMLab <a href="https://mp.weixin.qq.com/s/-_x4URDyhwqNC4mD_0daDw"
                target="_blank">[OpenMMLabç¤¾åŒºå¼€æ”¾éº¦]</a>, <a href="./files/OpenMMLabå¼€æ”¾éº¦.pdf" target="_blank">[slides]</a>,
              <a href="https://www.bilibili.com/video/BV1Wr4y1E7Qr" target="_blank">[bilibili playback]</a><img
                src="https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Wr4y1E7Qr" />,
              <a href="https://www.zhihu.com/zvideo/1529579010677370880" target="_blank">[zhihu playback]</a><img
                src="https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1529579010677370880" />
            </li>
            <li>â€ƒ â€¢ 06 / 2022: &nbsp; Young Scholars Forum of Wu Wenjun's artificial intelligence doctoral class <a
                href="https://mp.weixin.qq.com/s/lg1vNIZYqnDfq9SC93OT3A" target="_blank">[å´ç­Talk]</a>, <a
                href="./files/å´ç­Talk.pdf" target="_blank">[slides]</a></li>
            <li>â€ƒ â€¢ 12 / 2021: &nbsp; The fifth Jittor Forum <a href="https://mp.weixin.qq.com/s/8pYzCYU25B8Zzk78NBoovw"
                target="_blank">[ç¬¬äº”æœŸâ€œè®¡å›¾â€è®ºå›]</a>, <a href="./files/è®¡å›¾è®ºå›.pdf" target="_blank">[slides]</a></li>
            <li>â€ƒ â€¢ 05 / 2021: &nbsp; Magnolia Young Scholar Forum <a href="https://www.slidestalk.com/w/409"
                target="_blank">[ç™½ç‰å…°é’å¹´å­¦è€…è®ºå›]</a>, <a href="./files/æ—‹è½¬ç›®æ ‡æ£€æµ‹-ç™½ç‰å…°é’å¹´å­¦è€…è®ºå›.pdf" target="_blank">[slides]</a>
            </li>
          </ul>

        </div>
      </div>
    </div>


    <!--==========================================
                   Education
===========================================-->
    <div class="section education-section scrollspy" id="education">
      <div class="row container">
        <div class="row">
          <div class="title">ğŸ“ Education</div>
          <hr>
        </div>

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <a href="http://www.csu.edu.cn/" target="_blank">
              <img src="./images/csu.png" style="width:110px;height:110px;" align="center"
                class="img-responsive edu-img">
            </a>
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <p></p>
            <p></p>
            <p></p>
            <div class="degree"></div>
            <div class="degree"><b>B.E.</b> degree from School of Information Science and Engineering, <a
                href="http://www.csu.edu.cn/">Central South University</a>, Hunan, China</div>
            <div class="date">Sep. 2012 - July 2016</div>
          </div>
        </div>

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <a href="https://eece.ucas.ac.cn/index.php/zh-cn/" target="_blank">
              <img src="./images/ucas.png" style="width:110px;height:110px;" align="center"
                class="img-responsive edu-img">
              <!-- <img src="./images/iecas.jpeg" style="width:100px;height:100px;" align="center" class="img-responsive edu-img"> -->
              <!-- <img src="./images/air.png" style="width:100px;height:100px;" align="center" class="img-responsive edu-img"> -->
            </a>
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <p></p>
            <p></p>
            <p></p>
            <div class="degree"></div>
            <div class="degree"><b>M.S.</b> degree from <a href="https://eece.ucas.ac.cn/index.php/zh-cn/">School of
                Electronic, Electrical and Communication Engineering</a>, <a href="https://www.ucas.ac.cn/">University
                of Chinese Academy of
                Sciences</a><!--  (<a href="http://aircas.ac.cn/">Institute of Electrics, Chinese Academy of Sciences</a>) -->,
              Beijing, China</div>
            <div class="date">Sep. 2016 - July 2019</div>
          </div>
        </div>

        <div class="row">
          <div class="col s12 m12 l4 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <a href="https://www.sjtu.edu.cn/" target="_blank">
              <img src="./images/sjtu.png" style="width:110px;height:110px;" align="center"
                class="img-responsive edu-img">
            </a>
          </div>

          <div class="col s12 m12 l8 aos-init aos-animate" data-aos="fade-left" data-aos-offset="100"
            data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
            <p></p>
            <p></p>
            <p></p>
            <div class="degree"><b>Ph.D.</b> in CV, Department of Computer Science and Engineering</a>, <a
                href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, Shanghai, China</div>
            <div class="date">Sep. 2019 - July 2023</div>
          </div>
        </div>
      </div>
    </div>


    <div class="section preprints-section scrollspy" id="internship">

      <div class="row container">
        <div class="row">
          <div class="title">ğŸ§‘ğŸ»â€ğŸ’» Internship and Cooperation</div>
          <hr>
        </div>

        <div class="corp_item">
          <img class="corp_logo" style="height:80px;" src="./images/samsung.png"> &nbsp; &nbsp;
          <img class="corp_logo" style="height:80px;" src="./images/megvii.png"> &nbsp; &nbsp;
          <img class="corp_logo" style="height:80px;" src="./images/tencentmap.png"> &nbsp; &nbsp;
          <!-- <img class="corp_logo" style="height:110px;" src="./images/huawei.png"> -->
          <img class="corp_logo" style="height:80px;" src="./images/huawei1.png"> &nbsp; &nbsp;
          <img class="corp_logo" style="height:60px;" src="./images/pjlab.png"> &nbsp; &nbsp;
          <img class="corp_logo" style="height:80px;" src="./images/openmmlab.png"> &nbsp; &nbsp;
          <img class="corp_logo" style="height:80px;" src="./images/opengvlab.png">
        </div>

      </div>

    </div>


    <!-- ==========================================
                   Awards
=========================================== -->
    <div class="section awards-section scrollspy" id="awards">
      <div class="row container">
        <div class="row">
          <div class="title">ğŸ– Awards</div>
          <hr>
          <ul>
            <li>â€ƒ â€¢ <a href="https://mp.weixin.qq.com/s/sBczTcu0WN4nYpouOm5RMQ" target="_blank">The 10th Young Talent
                Support Project funded by CAST</a>, 2024</li>
            <li>â€ƒ â€¢ ä¸­å›½å›¾è±¡å›¾å½¢å­¦å­¦ä¼šè§†è§‰å¤§æ•°æ®ä¸“å§”ä¼š(CSIG-BVD)<a href="https://mp.weixin.qq.com/s/fhfizq577_-zPu8cGIAOxw"
                target="_blank">â€œæœåŠ¡è´¡çŒ®å¥–â€</a></li>
            <li>â€ƒ â€¢ <a href="https://www.gs.sjtu.edu.cn/yxbslw">SJTU Outstanding Doctoral Dissertation</a>, only fifteen
              winners in SJTU, 2023</li>
            <li>â€ƒ â€¢ <a href="https://mp.weixin.qq.com/s/Hk8iKReAUJVB_CraXqWYKg" target="_blank">CCF Outstanding Doctoral
                Dissertation Award</a>, only nine winners in China, 2023</li>
            <li>â€ƒ â€¢ <a href="https://topresearcherslist.com/Home/Profile/823455">World's Top 2% Scientists List</a>,
              2023-2024</li>
            <li>â€ƒ â€¢ Shanghai Outstanding Graduates, 2023</li>
            <li>â€ƒ â€¢ <a href="https://mp.weixin.qq.com/s/mUgpVmyvCHdo5-T6-u8Yxg">CCF-CV Academic Emerging Scholar</a>,
              only three winners in China, 2022</li>
            <li>â€ƒ â€¢ <a href="https://mp.weixin.qq.com/s/liVosHsotD2zDMyfmTIQJg" target="_blank">Doctoral National
                Scholarship</a>, 2022</li>
            <li>â€ƒ â€¢ Nominated by <a href="https://mp.weixin.qq.com/s/hr7qtx3OUffSGS9qUhqc9w" target="_blank">SJTU
                Scholar Star</a>, 10 official and 10 nomination awards in SJTU, 2021</li>
            <li>â€ƒ â€¢ Top 40 in the ninth Baidu Scholarship, 2021</li>
            <li>â€ƒ â€¢ <a href="https://mp.weixin.qq.com/s/ugk1l0QpuzJXaExJ_wH-4w" target="_blank">Doctoral National
                Scholarship</a>, Top-1 in CSE, 2021</li>
            <li>â€ƒ â€¢ <a href="https://ai.sjtu.edu.cn/info/news/126">Wu Wen Jun Honorary Doctoral Scholarship</a>, 2019
            </li>
            <li>â€ƒ â€¢ 1st place in <a href="https://gaia.didichuxing.com/d2city">WAD2019 Challenge</a> on the
              D<sup>2</sup>-City & BDD100K Detection Domain Adaption track, 2019</li>
            <li>â€ƒ â€¢ 3st/4th place in <a href="https://captain-whu.github.io/DOAI2019/results.html">DOAI2019
                Challenge</a> on the HBB/OBB track, 2019</li>
            <li>â€ƒ â€¢ Outstanding Student Leader, Outstanding Student, 2016-2019</li>
            <li>â€ƒ â€¢ Outstanding Student Leader, Outstanding Student, Outstanding Graduates, National Inspirational
              Scholarship, 2012-2016</li>
          </ul>
        </div>
      </div>
    </div>

    <!-- ==========================================
                   Projects
=========================================== -->
    <!-- <div class="section preprints-section scrollspy" id="projects">

  <div class="row container">
    <div class="row">
      <div class="title">ğŸ›  Projects</div>
      <hr>
    </div>

    <div class="row">
      <div class="col s12 m12 l9 center aos-init aos-animate" data-aos="fade-right" data-aos-offset="100" data-aos-easing="ease-in-sine" data-aos-duration="500" data-aos-once="true">
          <img class="responsive-img" src="./images/projects.png">
      </div>

      <div>
        <img class="responsive-img icon" src="./images/github.png">
        <a href="https://github.com/2112529" target="_blank">2112529</a><br>
      </div>

      <div>
        <img class="responsive-img icon" src="./images/github.png">
        <a href="https://github.com/DetectionTeamUCAS" target="_blank">DetectionTeamUCAS</a><br>
      </div>

      <div>
        <img class="responsive-img icon" src="./images/github.png">
        <a href="https://github.com/Thinklab-SJTU" target="_blank">Thinklab-SJTU</a><br>
      </div>

      <div>
        <img class="responsive-img icon" src="./images/github.png">
        <a href="https://github.com/SJTU-Thinklab-Det" target="_blank">SJTU-Thinklab-Det</a><br>
      </div>

      <div>
        <img class="responsive-img icon" src="./images/github.png">
        <a href="https://github.com/open-mmlab/mmrotate" target="_blank">open-mmlab/mmrotate</a><br>
      </div>

      <div>
        <img class="responsive-img icon" src="./images/github.png">
        <a href="https://github.com/Jittor/JDet" target="_blank">Jittor/JDet</a><br>
      </div>


    </div>
  </div>
</div>
 -->

    <!-- ==========================================
                   Demo
=========================================== -->
    <!-- <div class="section awards-section scrollspy" id="demos">
      <div class="row container">
        <div class="row">
          <div class="title">ğŸ’» Demos</div>
          <hr>
          <video width="640" height="480" controls muted>
            <source src="./videos/obsidian_minecraft.mp4" type="video/mp4">
          </video>
          <br>
          <video width="640" height="360" controls muted>
            <source
              src="https://user-images.githubusercontent.com/10410257/154433305-416d129b-60c8-44c7-9ebb-5ba106d3e9d5.MP4"
              type="video/mp4">
          </video>
          <img src="https://api.star-history.com/svg?repos=open-mmlab/mmrotate&type=Date" style="width:360px;" />
          <!-- <img src="https://api.star-history.com/svg?repos=2112529/RotationDetection&type=Date" style="width:600px;" /> -->
    <!-- <img src="https://api.star-history.com/svg?repos=open-mmlab/mmrotate&type=Date" style="width:600px;" /> -->
    <br>
    <!-- <video width="640" height="360" controls autoplay muted> -->
    <video width="640" height="360" controls muted>
      <source src="./videos/demo.mp4" type="video/mp4">
    </video>
    <img src="https://api.star-history.com/svg?repos=2112529/RotationDetection&type=Date" style="width:360px;" />
  </div>
  </div>
  </div> -->


  <!-- ==========================================
                   People
=========================================== -->
  <!-- <div class="section awards-section scrollspy" id="people">
      <div class="row container">
        <div class="row">
          <div class="title">ğŸ‘¬ People</div>
          <hr>
          <h5>Close Collaborators</h5>
          <li><a href="https://scholar.google.com/citations?user=v-aQ8GsAAAAJ&hl=en" target="_blank">Yue Zhou</a>
            <font size="1">(PostDoc., NTU)</font>, <a
              href="https://scholar.google.com/citations?user=OYtSc4AAAAAJ&hl=en" target="_blank">Yi Yu</a>
            <font size="1">(PostDoc., SEU)</font>, Gefan Zhang <font size="1">(Master, SJTU)</font>, <a href=""
              target="_blank">Wentao Wang</a>
            <font size="1">(PostDoc., PJLAB)</font>, <a
              href="https://scholar.google.com/citations?user=xO5QVEYAAAAJ&hl=en" target="_blank">Jirui Yang</a>
            <font size="1">(Master, UCAS)</font>, <a href="https://scholar.google.com/citations?user=zQCpqs8AAAAJ&hl=en"
              target="_blank">Qi Ming</a>
            <font size="1">(Ph.D., BIT)</font>, <a href="https://scholar.google.com/citations?user=MJjM6BcAAAAJ&hl=en"
              target="_blank">Wentong Li</a>
            <font size="1">(Ph.D., ZJU)</font>, <a
              href="https://scholar.google.com/citations?hl=zh-CN&user=6VsVHrAAAAAJ" target="_blank">Jiaqing Zhang</a>
            <font size="1">(Ph.D., Xidian)</font>
          </li>
          <li><a href="https://scholar.google.com/citations?user=JuRztWYAAAAJ&hl=en" target="_blank">Xuehui Wang</a>
            <font size="1">(Ph.D., SJTU)</font>, <a href="https://scholar.google.com/citations?user=TvsTun4AAAAJ&hl=en"
              target="_blank">Qingyun Li</a>
            <font size="1">(Ph.D., HIT)</font>, <a href="https://scholar.google.com/citations?user=rTG1yTQAAAAJ&hl=en"
              target="_blank">Tongkun Guan</a>
            <font size="1">(Ph.D., SJTU)</font>, <a href="https://scholar.google.com/citations?hl=en&user=6XibZaYAAAAJ"
              target="_blank">Junwei Luo</a>
            <font size="1">(Master, WHU)</font>, <a
              href="https://scholar.google.com/citations?hl=zh-CN&user=W0zVf-oAAAAJ" target="_blank">Zhaokai Wang</a>
            <font size="1">(Ph.D., SJTU & PJLAB)</font>, Botao Ren <font size="1">(Ph.D., Tsinghua)</font>
          </li>

          <h5>Students</h5>
          <li>Yuan Liu <font size="1">(Undergraduate, SJTU, 2021-)</font>, Peiyuan Zhang <font size="1">(Undergraduate,
              WHU, 2022-)</font>
          </li>
          <li><a href="https://scholar.google.com/citations?user=1JSLzoIAAAAJ&hl=en" , target="_blank"> Yifan Zhou</a>
            <font size="1">(Master, SJTU, 2024-)</font>, Wei Zhang <font size="1">(Master, SJTU, 2024-)</font>, Mingxin
            Liu <font size="1">(Master, SJTU, 2024-)</font>
          </li>
          <li>Qipeng Liu <font size="1">(Ph.D. SJTU, 2024-)</font>
          </li>

        </div>
      </div>
    </div> -->


  <!--==========================================
                   Footer
===========================================-->
  <footer class="page-footer grey lighten-2">
    <div class="row">
      <div class="widgetContainer" style="width:300px; margin: 0 auto;">
        <script type='text/javascript' id='clustrmaps'
          src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=yZcblN50sSwsCOVmEPYqkPD6Wo-RFHx0E2yb6Ktm_Wk&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353'></script>
      </div>
    </div>
    <div class="footer-copyright center black-text">
      Copyright Â© Ting-Feng Zhao 2025
    </div>
  </footer>

  <!--  Scripts-->

  <!-- <script src="./files/jquery-2.1.1.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
  <script src="./files/materialize.js"></script>
  <script src="./files/aos.js"></script>
  <script src="./init.js"></script>
  <!-- <script>
      // window.onload = function () {
      $(document).ready(function () {
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/2112529/2112529.github.io/'
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {

          //var totalCitation = data['citedby']
          //document.getElementById('total_cit').innerHTML = totalCitation;
          var citationEles = document.getElementsByClassName('show_paper_citations')
          Array.prototype.forEach.call(citationEles, element => {
            var paperId = element.getAttribute('data')
            var numCitations = data['publications'][paperId]['num_citations']
            element.innerHTML = numCitations;
          });
        });
      });
    </script> -->

  <style>
    #dify-chatbot-bubble-button {
      background-color: #1C64F2 !important;
    }

    #dify-chatbot-bubble-window {
      width: 24rem !important;
      height: 40rem !important;
    }
  </style>

  <script>
    window.difyChatbotConfig = {
      token: 'ul7V4zpZTlTe3thZ'
    }
  </script>
  <script src="https://udify.app/embed.min.js" id="ul7V4zpZTlTe3thZ" defer>
  </script>


  </div>
  <div class="jvectormap-tip"></div>
</body>

</html>